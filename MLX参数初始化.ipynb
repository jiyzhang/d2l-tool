{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLX参数初始化文档\n",
    "\n",
    "在MLX框架中，参数初始化是神经网络训练中至关重要的一部分。本文档将总结MLX框架中常用的参数初始化方法，并结合用户提供的方法进行详细介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx\n",
    "from mlx.utils import tree_flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数初始化方法概述\n",
    "\n",
    "在神经网络中，合适的权重初始化可以加速训练收敛并提高模型性能。MLX框架提供了一些常用的初始化方法，如Xavier初始化（Glorot初始化）等。\n",
    "在绝大部分情况下，mlx可以使用两种不同的方法进行参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 直接通过net遍历：\n",
    "对于一个简单神经网络(无嵌套网络结构)，我们可以直接遍历网络到需要初始化的层，用内置的出实话函数进行参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (layers.0): Linear(input_dims=4, output_dims=8, bias=True)\n",
      "  (layers.1): ReLU()\n",
      "  (layers.2): Linear(input_dims=8, output_dims=1, bias=True)\n",
      ")\n",
      "array([[0.422567, 0.130482, 0.59768, -0.0286205],\n",
      "       [-0.634305, -0.440786, -0.23919, -0.407694],\n",
      "       [0.571524, 0.254098, -0.443719, -0.519819],\n",
      "       ...,\n",
      "       [-0.0761608, -0.38404, -0.225917, 0.321595],\n",
      "       [-0.117003, -0.095284, -0.260199, -0.0796262],\n",
      "       [-0.427804, -0.493432, -0.617128, -0.46343]], dtype=float32)\n",
      "array([[42, 42, 42, ..., 42, 42, 42]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 定义神经网络\n",
    "net = nn.Sequential(nn.Linear(4, 8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "\n",
    "print(net)\n",
    "\n",
    "# 通过索引进行参数初始化\n",
    "uniform_fn = nn.init.glorot_uniform()\n",
    "const_fn = nn.init.constant(42.0)\n",
    "net.layers[0].weight = uniform_fn(net.layers[0].weight)\n",
    "net.layers[2].weight = const_fn(net.layers[2].weight)\n",
    "print(net.layers[0].weight)\n",
    "print(net.layers[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数初始化函数概述\n",
    "\n",
    "在神经网络中，合适的权重初始化可以加速训练收敛并提高模型性能。MLX框架提供了一些常用的初始化方法，如Xavier初始化（Glorot初始化）等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Constant初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1], dtype=float32), array(0, dtype=float32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_fn = nn.init.constant(1.0)\n",
    "bias_fn = nn.init.constant(0.0)\n",
    "for layer in net.layers:\n",
    "    if type(layer) == nn.Linear:\n",
    "        layer.weight = weight_fn(layer.weight)\n",
    "        layer.bias = bias_fn(layer.bias)\n",
    "\n",
    "net.layers[0].weight[0], net.layers[0].bias[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uniform/Normal初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.0146943, 0.00767959, -0.000236169, 0.000649628], dtype=float32),\n",
       " array(0.440777, dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义神经网络\n",
    "net = nn.Sequential(nn.Linear(4, 8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "\n",
    "weight_fn = nn.init.normal(mean=0.0, std=0.01)\n",
    "for layer in net.layers:\n",
    "    if type(layer) == nn.Linear:\n",
    "        layer.weight = weight_fn(layer.weight)\n",
    "\n",
    "net.layers[0].weight[0], net.layers[0].bias[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Xavier初始化\n",
    "Xavier初始化所接收的参数必须dim > 1. 所以Xavier函数不能用于对bias的初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0.0795656, 0.355053, -0.316831, 0.596846], dtype=float32)\n",
      "array([[42, 42, 42, ..., 42, 42, 42]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "uniform_fn = nn.init.glorot_uniform()\n",
    "const_fn = nn.init.constant(42.0)\n",
    "net.layers[0].weight = uniform_fn(net.layers[0].weight)\n",
    "net.layers[2].weight = const_fn(net.layers[2].weight)\n",
    "print(net.layers[0].weight[0])\n",
    "print(net.layers[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.使用Modules函数遍历所有层：\n",
    "对于一个复杂神经网络，我们可以使用内置的函数module.update方法来进行参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 相关函数介绍：\n",
    "初始化函数大多使用module.update/module.update_modules\n",
    "\n",
    "module.update：\n",
    "- 该函数需要传入一个dict类型的参数作为input，会将该dict中所有的参数更新到当前的module上\n",
    "- 大多数情况下要和mlx.utils.tree_map配合使用\n",
    "- mlx.utils.tree_map需要传入两个参数，一个为map function，另一个为map function的对象，类型为dictionary。该函数返回一个dictionary类型，内容为将map_fn的内容映射到map function的对象dict上后的结果。\n",
    "\n",
    "module.update_modules：\n",
    "- 与module.update及其类似\n",
    "- 唯一区别是module.update会更新当前神经网络中的所有参数，但module.update_modules只会更新所提供的module的参数，例如module==nn.linear，则只会更新linear层中的参数\n",
    "\n",
    "tree_flatten()：\n",
    "- 接收输入为一个python tree（神经网络）或一个dictionary（model.parameters）\n",
    "- 输出一个展平后的神经网络结构，类型为list，内容是key和value的tuple\n",
    "- The keys are using the dot notation to define trees of arbitrary depth and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block3():\n",
    "    return nn.Sequential(nn.Linear(2, 3), nn.ReLU(),\n",
    "                         nn.Linear(1, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = []\n",
    "    for i in range(2):\n",
    "        # 在这里嵌套\n",
    "        net.append(block1())\n",
    "    for i in range(2):\n",
    "        # 在这里嵌套\n",
    "        net.append(block3())\n",
    "    return net\n",
    "\n",
    "net = nn.Sequential(*block2(), nn.Linear(4, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (layers.0): Sequential(\n",
      "    (layers.0): Linear(input_dims=4, output_dims=8, bias=True)\n",
      "    (layers.1): ReLU()\n",
      "    (layers.2): Linear(input_dims=8, output_dims=4, bias=True)\n",
      "    (layers.3): ReLU()\n",
      "  )\n",
      "  (layers.1): Sequential(\n",
      "    (layers.0): Linear(input_dims=4, output_dims=8, bias=True)\n",
      "    (layers.1): ReLU()\n",
      "    (layers.2): Linear(input_dims=8, output_dims=4, bias=True)\n",
      "    (layers.3): ReLU()\n",
      "  )\n",
      "  (layers.2): Sequential(\n",
      "    (layers.0): Linear(input_dims=2, output_dims=3, bias=True)\n",
      "    (layers.1): ReLU()\n",
      "    (layers.2): Linear(input_dims=1, output_dims=4, bias=True)\n",
      "    (layers.3): ReLU()\n",
      "  )\n",
      "  (layers.3): Sequential(\n",
      "    (layers.0): Linear(input_dims=2, output_dims=3, bias=True)\n",
      "    (layers.1): ReLU()\n",
      "    (layers.2): Linear(input_dims=1, output_dims=4, bias=True)\n",
      "    (layers.3): ReLU()\n",
      "  )\n",
      "  (layers.4): Linear(input_dims=4, output_dims=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)\n",
    "#只打印最外层结构 无法访问每一层参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以按照层级结构遍历访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42, 42, 42, ..., 42, 42, 42],\n",
       "       [42, 42, 42, ..., 42, 42, 42],\n",
       "       [42, 42, 42, ..., 42, 42, 42],\n",
       "       [42, 42, 42, ..., 42, 42, 42]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[1].layers[2].weight\n",
    "\n",
    "net.layers[1].layers[2].weight = const_fn(net.layers[1].layers[2].weight)\n",
    "\n",
    "net.layers[1].layers[2].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sequential(\n",
      "  (layers.0): Sequential(\n",
      "    (layers.0): Linear(input_dims=4, output_dims=8, bias=True)\n",
      "    (layers.1): ReLU()\n",
      "    (layers.2): Linear(input_dims=8, output_dims=4, bias=True)\n",
      "    (layers.3): ReLU()\n",
      "  )\n",
      "  (layers.1): Sequential(\n",
      "    (layers.0): Linear(input_dims=4, output_dims=8, bias=True)\n",
      "    (layers.1): ReLU()\n",
      "    (layers.2): Linear(input_dims=8, output_dims=4, bias=True)\n",
      "    (layers.3): ReLU()\n",
      "  )\n",
      "  (layers.2): Sequential(\n",
      "    (layers.0): Linear(input_dims=2, output_dims=3, bias=True)\n",
      "    (layers.1): ReLU()\n",
      "    (layers.2): Linear(input_dims=1, output_dims=4, bias=True)\n",
      "    (layers.3): ReLU()\n",
      "  )\n",
      "  (layers.3): Sequential(\n",
      "    (layers.0): Linear(input_dims=2, output_dims=3, bias=True)\n",
      "    (layers.1): ReLU()\n",
      "    (layers.2): Linear(input_dims=1, output_dims=4, bias=True)\n",
      "    (layers.3): ReLU()\n",
      "  )\n",
      "  (layers.4): Linear(input_dims=4, output_dims=1, bias=True)\n",
      "), Linear(input_dims=4, output_dims=1, bias=True), Sequential(\n",
      "  (layers.0): Linear(input_dims=2, output_dims=3, bias=True)\n",
      "  (layers.1): ReLU()\n",
      "  (layers.2): Linear(input_dims=1, output_dims=4, bias=True)\n",
      "  (layers.3): ReLU()\n",
      "), ReLU(), Linear(input_dims=1, output_dims=4, bias=True), ReLU(), Linear(input_dims=2, output_dims=3, bias=True), Sequential(\n",
      "  (layers.0): Linear(input_dims=2, output_dims=3, bias=True)\n",
      "  (layers.1): ReLU()\n",
      "  (layers.2): Linear(input_dims=1, output_dims=4, bias=True)\n",
      "  (layers.3): ReLU()\n",
      "), ReLU(), Linear(input_dims=1, output_dims=4, bias=True), ReLU(), Linear(input_dims=2, output_dims=3, bias=True), Sequential(\n",
      "  (layers.0): Linear(input_dims=4, output_dims=8, bias=True)\n",
      "  (layers.1): ReLU()\n",
      "  (layers.2): Linear(input_dims=8, output_dims=4, bias=True)\n",
      "  (layers.3): ReLU()\n",
      "), ReLU(), Linear(input_dims=8, output_dims=4, bias=True), ReLU(), Linear(input_dims=4, output_dims=8, bias=True), Sequential(\n",
      "  (layers.0): Linear(input_dims=4, output_dims=8, bias=True)\n",
      "  (layers.1): ReLU()\n",
      "  (layers.2): Linear(input_dims=8, output_dims=4, bias=True)\n",
      "  (layers.3): ReLU()\n",
      "), ReLU(), Linear(input_dims=8, output_dims=4, bias=True), ReLU(), Linear(input_dims=4, output_dims=8, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "print(net.modules())\n",
    "#DFS遍历所有层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m         layer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m weight_fn(layer\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m      7\u001b[0m         layer\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m bias_fn(layer\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m[\u001b[38;5;241m0\u001b[39m], net\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbias[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx/lib/python3.10/site-packages/mlx/nn/layers/base.py:137\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mModule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "#直接遍历只能访问最外层结构 无法初始化内部层\n",
    "weight_fn = nn.init.constant(1.0)\n",
    "bias_fn = nn.init.constant(0.0)\n",
    "for layer in net.layers:\n",
    "    if type(layer) == nn.Linear:\n",
    "        layer.weight = weight_fn(layer.weight)\n",
    "        layer.bias = bias_fn(layer.bias)\n",
    "\n",
    "net.layers[0].weight[0], net.layers[0].bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before_init: [('layers.0.layers.0.weight', array([[0.211076, -0.0322273, -0.171056, 0.462113],\n",
      "       [-0.462399, 0.399972, -0.42979, -0.42679],\n",
      "       [0.337287, -0.0173589, 0.442565, -0.209292],\n",
      "       ...,\n",
      "       [0.384921, 0.381924, 0.303233, 0.486555],\n",
      "       [0.0861257, -0.25014, 0.278124, -0.434768],\n",
      "       [-0.243735, 0.183608, -0.0420352, -0.391986]], dtype=float32)), ('layers.0.layers.0.bias', array([-0.252933, -0.285083, 0.299472, ..., -0.246823, -0.460196, 0.431308], dtype=float32)), ('layers.0.layers.2.weight', array([[0.286649, -0.279438, -0.0777029, ..., 0.104014, -0.26662, 0.342589],\n",
      "       [-0.262362, 0.301246, -0.0589984, ..., -0.0613111, -0.123374, -0.263305],\n",
      "       [-0.191364, 0.0707401, 0.317304, ..., -0.262486, -0.270887, -0.315851],\n",
      "       [0.215008, 0.346379, 0.146354, ..., -0.0982642, -0.0868911, -0.188508]], dtype=float32)), ('layers.0.layers.2.bias', array([-0.140886, 0.053999, 0.313642, -0.300036], dtype=float32)), ('layers.1.layers.0.weight', array([[-0.262482, 0.1395, -0.241195, -0.0748574],\n",
      "       [-0.105827, 0.0420644, 0.401716, 0.432168],\n",
      "       [0.137154, -0.0686379, -0.24993, -0.414013],\n",
      "       ...,\n",
      "       [0.495958, 0.19829, -0.121326, 0.273927],\n",
      "       [0.211712, 0.0731277, 0.145148, 0.02908],\n",
      "       [-0.0789994, -0.245508, -0.18102, -0.0778]], dtype=float32)), ('layers.1.layers.0.bias', array([-0.095374, -0.0571319, 0.255056, ..., -0.252903, 0.130312, -0.345524], dtype=float32)), ('layers.1.layers.2.weight', array([[42, 42, 42, ..., 42, 42, 42],\n",
      "       [42, 42, 42, ..., 42, 42, 42],\n",
      "       [42, 42, 42, ..., 42, 42, 42],\n",
      "       [42, 42, 42, ..., 42, 42, 42]], dtype=float32)), ('layers.1.layers.2.bias', array([0.0596042, -0.0634729, 0.323755, 0.290034], dtype=float32)), ('layers.2.layers.0.weight', array([[0.392848, 0.0459214],\n",
      "       [0.0180684, -0.0421997],\n",
      "       [0.183748, 0.216597]], dtype=float32)), ('layers.2.layers.0.bias', array([-0.403882, -0.185939, -0.180016], dtype=float32)), ('layers.2.layers.2.weight', array([[0.063704],\n",
      "       [-0.604203],\n",
      "       [-0.98294],\n",
      "       [-0.251784]], dtype=float32)), ('layers.2.layers.2.bias', array([0.189216, -0.0390911, 0.498517, -0.604759], dtype=float32)), ('layers.3.layers.0.weight', array([[0.493464, 0.433557],\n",
      "       [0.681213, -0.0278865],\n",
      "       [-0.336061, 0.120381]], dtype=float32)), ('layers.3.layers.0.bias', array([-0.124146, -0.599365, -0.494733], dtype=float32)), ('layers.3.layers.2.weight', array([[-0.672312],\n",
      "       [0.164224],\n",
      "       [-0.59447],\n",
      "       [-0.295474]], dtype=float32)), ('layers.3.layers.2.bias', array([-0.623483, -0.47791, 0.273555, 0.114087], dtype=float32)), ('layers.4.weight', array([[1, 1, 1, 1]], dtype=float32)), ('layers.4.bias', array([0], dtype=float32))]\n",
      "after init: [('layers.0.layers.0.weight', array([[10, 10, 10, 10],\n",
      "       [10, 10, 10, 10],\n",
      "       [10, 10, 10, 10],\n",
      "       ...,\n",
      "       [10, 10, 10, 10],\n",
      "       [10, 10, 10, 10],\n",
      "       [10, 10, 10, 10]], dtype=float32)), ('layers.0.layers.0.bias', array([1, 1, 1, ..., 1, 1, 1], dtype=float32)), ('layers.0.layers.2.weight', array([[10, 10, 10, ..., 10, 10, 10],\n",
      "       [10, 10, 10, ..., 10, 10, 10],\n",
      "       [10, 10, 10, ..., 10, 10, 10],\n",
      "       [10, 10, 10, ..., 10, 10, 10]], dtype=float32)), ('layers.0.layers.2.bias', array([1, 1, 1, 1], dtype=float32)), ('layers.1.layers.0.weight', array([[10, 10, 10, 10],\n",
      "       [10, 10, 10, 10],\n",
      "       [10, 10, 10, 10],\n",
      "       ...,\n",
      "       [10, 10, 10, 10],\n",
      "       [10, 10, 10, 10],\n",
      "       [10, 10, 10, 10]], dtype=float32)), ('layers.1.layers.0.bias', array([1, 1, 1, ..., 1, 1, 1], dtype=float32)), ('layers.1.layers.2.weight', array([[10, 10, 10, ..., 10, 10, 10],\n",
      "       [10, 10, 10, ..., 10, 10, 10],\n",
      "       [10, 10, 10, ..., 10, 10, 10],\n",
      "       [10, 10, 10, ..., 10, 10, 10]], dtype=float32)), ('layers.1.layers.2.bias', array([1, 1, 1, 1], dtype=float32)), ('layers.2.layers.0.weight', array([[10, 10],\n",
      "       [10, 10],\n",
      "       [10, 10]], dtype=float32)), ('layers.2.layers.0.bias', array([1, 1, 1], dtype=float32)), ('layers.2.layers.2.weight', array([[10],\n",
      "       [10],\n",
      "       [10],\n",
      "       [10]], dtype=float32)), ('layers.2.layers.2.bias', array([1, 1, 1, 1], dtype=float32)), ('layers.3.layers.0.weight', array([[10, 10],\n",
      "       [10, 10],\n",
      "       [10, 10]], dtype=float32)), ('layers.3.layers.0.bias', array([1, 1, 1], dtype=float32)), ('layers.3.layers.2.weight', array([[10],\n",
      "       [10],\n",
      "       [10],\n",
      "       [10]], dtype=float32)), ('layers.3.layers.2.bias', array([1, 1, 1, 1], dtype=float32)), ('layers.4.weight', array([[10, 10, 10, 10]], dtype=float32)), ('layers.4.bias', array([1], dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "# 定义一个初始化参数方法\n",
    "def init_weights(array):\n",
    "    if array.ndim > 1:\n",
    "        weight_fn = nn.init.constant(10)\n",
    "        array = weight_fn(array)\n",
    "    else:\n",
    "        bias_fn = nn.init.constant(1)\n",
    "        array = bias_fn(array)\n",
    "    return array\n",
    "\n",
    "#通过modules DFS遍历 所有层级，并通过module.update()函数来更新对应参数\n",
    "def apply_initialization(models, init_fn):\n",
    "    for module in models.modules():\n",
    "        if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
    "            module.update(mlx.utils.tree_map(lambda x: init_fn(x), module.parameters()))\n",
    "\n",
    "print(\"before_init:\", tree_flatten(net.parameters()) )\n",
    "apply_initialization(net, init_weights)\n",
    "print(\"after init:\", tree_flatten(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('layers.0.layers.0.weight',\n",
       "  array([[10, 10, 10, 10],\n",
       "         [10, 10, 10, 10],\n",
       "         [10, 10, 10, 10],\n",
       "         ...,\n",
       "         [10, 10, 10, 10],\n",
       "         [10, 10, 10, 10],\n",
       "         [10, 10, 10, 10]], dtype=float32)),\n",
       " ('layers.0.layers.0.bias', array([1, 1, 1, ..., 1, 1, 1], dtype=float32)),\n",
       " ('layers.0.layers.2.weight',\n",
       "  array([[10, 10, 10, ..., 10, 10, 10],\n",
       "         [10, 10, 10, ..., 10, 10, 10],\n",
       "         [10, 10, 10, ..., 10, 10, 10],\n",
       "         [10, 10, 10, ..., 10, 10, 10]], dtype=float32)),\n",
       " ('layers.0.layers.2.bias', array([1, 1, 1, 1], dtype=float32)),\n",
       " ('layers.1.layers.0.weight',\n",
       "  array([[10, 10, 10, 10],\n",
       "         [10, 10, 10, 10],\n",
       "         [10, 10, 10, 10],\n",
       "         ...,\n",
       "         [10, 10, 10, 10],\n",
       "         [10, 10, 10, 10],\n",
       "         [10, 10, 10, 10]], dtype=float32)),\n",
       " ('layers.1.layers.0.bias', array([1, 1, 1, ..., 1, 1, 1], dtype=float32)),\n",
       " ('layers.1.layers.2.weight',\n",
       "  array([[10, 10, 10, ..., 10, 10, 10],\n",
       "         [10, 10, 10, ..., 10, 10, 10],\n",
       "         [10, 10, 10, ..., 10, 10, 10],\n",
       "         [10, 10, 10, ..., 10, 10, 10]], dtype=float32)),\n",
       " ('layers.1.layers.2.bias', array([1, 1, 1, 1], dtype=float32)),\n",
       " ('layers.2.layers.0.weight',\n",
       "  array([[10, 10],\n",
       "         [10, 10],\n",
       "         [10, 10]], dtype=float32)),\n",
       " ('layers.2.layers.0.bias', array([1, 1, 1], dtype=float32)),\n",
       " ('layers.2.layers.2.weight',\n",
       "  array([[10],\n",
       "         [10],\n",
       "         [10],\n",
       "         [10]], dtype=float32)),\n",
       " ('layers.2.layers.2.bias', array([1, 1, 1, 1], dtype=float32)),\n",
       " ('layers.3.layers.0.weight',\n",
       "  array([[10, 10],\n",
       "         [10, 10],\n",
       "         [10, 10]], dtype=float32)),\n",
       " ('layers.3.layers.0.bias', array([1, 1, 1], dtype=float32)),\n",
       " ('layers.3.layers.2.weight',\n",
       "  array([[10],\n",
       "         [10],\n",
       "         [10],\n",
       "         [10]], dtype=float32)),\n",
       " ('layers.3.layers.2.bias', array([1, 1, 1, 1], dtype=float32)),\n",
       " ('layers.4.weight', array([[10, 10, 10, 10]], dtype=float32)),\n",
       " ('layers.4.bias', array([1], dtype=float32))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_flatten(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结论\n",
    "\n",
    "参数初始化在神经网络训练中扮演着重要角色，合适的初始化方法可以显著提高模型性能。MLX框架提供了多种初始化工具，用户可以结合具体需求选择合适的方法，并通过自定义函数进一步优化初始化过程。通过本文档的介绍，希望用户能更好地理解并应用MLX框架中的参数初始化技术。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
